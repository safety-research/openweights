import math
import json
import os
import torch
import torch.nn.functional as F
from transformers import TrainerCallback
from utils import client


class MCQCallback(TrainerCallback):
    def __init__(self, mc_eval, tokenizer, eval_steps='log', batch_size=8, tag='mcq'):
        """
        A callback that evaluates model performance on a test dataset and logs the results.
        
        Args:
            test_dataset: Dataset with 'messages' field containing conversation messages
            tokenizer: The tokenizer to use for encoding conversations
            eval_steps: Evaluate every `eval_steps` training steps
            output_dir: Directory where token-level logP data will be saved
            batch_size: Batch size to use during evaluation
            log_as: Key to use when logging the loss metric
        """
        self.mc_eval = mc_eval
        self.tokenizer = tokenizer
        self.eval_steps = eval_steps
        self.batch_size = batch_size
        self.tag = tag
        
        os.environ['UNSLOTH_RETURN_LOGITS'] = '1'

    def on_step_end(self, args, state, control, **kwargs):
        """Called at the end of each training step."""
        print(f"Step {state.global_step}")
        eval_steps = 10 ** int(math.log10(max(1, state.global_step)))
        if self.eval_steps == 'log':
            eval_steps = eval_steps
        else:
            eval_steps = min(eval_steps, self.eval_steps)
        print(f"Evaluating every {eval_steps} steps")
        
        if state.global_step % eval_steps != 0:
            return
            
        # Get the model from kwargs
        model = kwargs["model"]
        
        # Set model to eval mode
        model.eval()
        
        metrics = self.mc_eval.get_metrics(model, self.tokenizer, self.batch_size)
        metrics['tag'] = self.tag
        metrics['type'] = 'mc'
        metrics['step'] = state.global_step
        # Log the test loss
        client.run.log(metrics)

        # Return model to training mode
        model.train()
