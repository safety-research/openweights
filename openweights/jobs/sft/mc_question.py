from typing import List, Dict, Union
from dataclasses import dataclass
from enum import Enum
import random
import hashlib
from collections import defaultdict
import pandas as pd
import os

from datasets import Dataset
import numpy as np

from logprobs import get_logprobs_blockwise
from utils import client, load_model_and_tokenizer
from validate import MCQJobModel, ContentBlockMessage


@dataclass
class Choice:
    text: str
    is_correct: bool


class Question:
    def __init__(
        self, question: str,
        choices: List[Choice],
        id: str | None = None, 
        choice_template=None,
        question_template=None,
        answer_template=None,
        context=[],
        meta={}
    ):
        self.question = question
        self.choices = choices
        self.id = id or hashlib.sha256(question.encode()).hexdigest()
        self.choice_template = choice_template
        self.question_template = question_template
        self.answer_template = answer_template
        self.context = context
        meta={}
    
    def prepare(
        self,
        choice_template='{choice_char}: {choice_text}',
        question_template='{question_text}\n{choices_text}',
        answer_template=[{
            'type': 'text',
            'text': '{choice_char}',
            'logprobs': True,
        }],
        context=[],
        only_correct=False
    ):
        choice_template = self.choice_template or choice_template
        question_template = self.question_template or question_template
        answer_template = self.answer_template or answer_template
        context = self.context or context
        context = [q.model_dump() if isinstance(q, ContentBlockMessage) else q for q in context]

        choices_text = '\n'.join([
            choice_template.format(choice_char=chr(65 + i), choice_text=choice.text)
            for i, choice in enumerate(self.choices)
        ])
        question_text = question_template.format(question_text=self.question, choices_text=choices_text)
        batch = []

        def apply_to_content_block(i, choice_char, choice_text):
            text = i['text'].format(choice_char=choice_char, choice_text=choice_text)
            return dict(i, text=text)

        for i, choice in enumerate(self.choices):
            if only_correct and not choice.is_correct:
                continue
            batch.append(dict(id=self.id, question_text=self.question, choice_text=choice.text, is_correct=choice.is_correct, messages=context + [
                {
                    'role': 'user',
                    'content': [
                        {
                            'type': 'text',
                            'text': question_text,
                            'logprobs': False
                        }
                    ]
                },
                {
                    'role': 'assistant',
                    'content': [
                        apply_to_content_block(block, choice_char=chr(65 + i), choice_text=choice.text)
                        for block in answer_template
                    ]
                }
            ]))
        return batch



class MultipleChoiceEval:
    def __init__(self, questions, choice_template: str, question_template: str, answer_template: List[Dict[str, Union[str, bool]]], context=[], randomize=True):
        self.questions : List[Question] = questions
        self.choice_template = choice_template
        self.question_template = question_template
        self.answer_template = answer_template
        self.context = context
        if randomize:
            self.randomize()
    
    def randomize(self):
        """Randomize the order of choices"""
        for question in self.questions:
            random.shuffle(question.choices)
    
    def as_messages(self, only_correct=False) -> List[Dict]:
        messages = []
        for question in self.questions:
            messages.extend(question.prepare(self.choice_template, self.question_template, self.answer_template, self.context, only_correct))
        return messages

    def get_logprobs(self, model, tokenizer, batch_size=4):
        conversations = self.as_messages()
        logprobs = get_logprobs_blockwise(model, tokenizer, conversations, batch_size=batch_size)
        return logprobs
    
    def _sum_over_blocks(self, example):
        return sum(
            sum(
                block['logprobs']
                for block in message['content'] if block['logprobs'] is not False)
            for message in example['messages']
        )

    def get_metrics(self, model, tokenizer, batch_size):
        logprob_results = self.get_logprobs(model, tokenizer, batch_size)

        questions = defaultdict(list)
        for example in logprob_results:
            questions[example['id']].append(example)
        
        question_results = []
        
        for question_id, examples in questions.items():
            # Get the total logprob for each choice
            choice_scores = []
            for example in examples:
                # Access the logprobs from the last message (assistant's response)
                total_logprob = self._sum_over_blocks(example)
                choice_scores.append({
                    'is_correct': example['is_correct'],
                    'logprob': total_logprob,
                    'choice_text': example['choice_text']
                })
                if example['is_correct']:
                    logp_correct = total_logprob
            
            # Find the choice with the highest logprob
            max_logprob_idx = np.argmax([choice['logprob'] for choice in choice_scores])
            predicted_correct = choice_scores[max_logprob_idx]['is_correct']
            
            # Store results for this question
            p_correct = np.exp(logp_correct)
            p_any_choice = np.exp([choice['logprob'] for choice in choice_scores]).sum()
            question_results.append({
                'id': question_id,
                'question_text': example['question_text'],
                'correct': predicted_correct,
                'logp_correct': logp_correct,
                'p_correct': p_correct,
                'p_any_choice': p_any_choice,
                'p_correct|any_choice': p_correct / p_any_choice,
                'choices': choice_scores
            })
        
        questions_df = pd.DataFrame(question_results)
        metrics = {
            'accuracy': questions_df.correct.mean(),
            'logp_correct': questions_df.logp_correct.mean(),
            'p_correct': questions_df.p_correct.mean(),
            'p_any_choice': questions_df.p_any_choice.mean(),
            'p_correct|any_choice': questions_df['p_correct|any_choice'].mean(),
            'df': question_results
        }
        return metrics


class MultipleChoiceEvalABC(MultipleChoiceEval):
    def __init__(
        self,
        questions,
        choice_template='{choice_char}: {choice_text}',
        question_template='{question_text}\n{choices_text}',
        answer_template=[{
            'type': 'text',
            'text': '{choice_char}',
            'logprobs': True,
        }],
        context=[]
    ):
        super().__init__(questions, choice_template, question_template, answer_template, context)


class MultipleChoiceEvalFreeform(MultipleChoiceEval):
    def __init__(
        self,
        questions,
        choice_template='{choice_text}',
        question_template='{question_text}',
        answer_template=[{
            'type': 'text',
            'text': '{choice_text}',
            'logprobs': True,
        }],
        context=[]
    ):
        super().__init__(questions, choice_template, question_template, answer_template, context)


def main(config_job_id: str):
    os.environ['UNSLOTH_RETURN_LOGITS'] = '1'
    if os.path.exists(config_job_id):
        with open(config, 'r') as f:
            config = json.load(f)
    else:
        job = client.jobs.retrieve(config_job_id)
        config = job['params']['validated_params']
    
    job = MCQJobModel(**config)
    mc_eval = job.mc_eval.to_eval()
    model, tokenizer = load_model_and_tokenizer(job.model)
    metrics = mc_eval.get_metrics(model, tokenizer, job.batch_size)
    client.log(metrics)


if __name__ == "__main__":
    import sys
    main(sys.argv[1])